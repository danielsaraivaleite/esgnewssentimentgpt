{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03cde1f2",
   "metadata": {},
   "source": [
    "# Construção da base de notícias ESG a partir da captura do Google RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bd9fa",
   "metadata": {},
   "source": [
    "- Autor: Daniel Saraiva Leite - 2023\n",
    "- Projeto Análise de sentimentos sobre notícias do tema ESG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5cac40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from noticias_google_buscador import busca_noticias_google_news\n",
    "from noticias_google_buscador_esg import *\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "arquivo_termos_esg = 'datasets/palavras_chave_esg.xlsx'\n",
    "base_noticias = 'datasets/base_noticias.xlsx'\n",
    "lista_empresas = 'datasets/lista_empresas.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f1ebec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-02-25 21:56:45.378783 Buscando empresa bahema\n",
      "...... buscando no google bahema\n",
      "2024-02-25 21:57:25.077632 ... noticias encontradas: 1\n",
      "2024-02-25 21:57:25.086397 ... noticias novas: 1\n",
      "2024-02-25 21:57:27.417002 ... textos buscados: 1\n",
      "2024-02-25 21:57:27.563984 ... novas noticias após filtros: 1\n",
      "\n",
      "2024-02-25 21:57:41.536372 Buscando empresa gafisa\n",
      "...... buscando no google gafisa\n",
      "2024-02-25 21:57:44.597361 ... noticias encontradas: 1\n",
      "2024-02-25 21:57:44.605979 ... noticias novas: 1\n",
      "2024-02-25 21:57:45.738078 ... textos buscados: 0\n",
      "\n",
      "2024-02-25 21:57:45.738683 Buscando empresa itau unibanco\n",
      "...... buscando no google itau+unibanco\n",
      "2024-02-25 21:57:48.242661 ... noticias encontradas: 15\n",
      "2024-02-25 21:57:48.250585 ... noticias novas: 14\n",
      "2024-02-25 21:58:15.832538 ... textos buscados: 14\n",
      "2024-02-25 21:58:17.256383 ... novas noticias após filtros: 14\n",
      "\n",
      "2024-02-25 21:58:31.231932 Buscando empresa magazine luiza\n",
      "...... buscando no google magazine+luiza\n",
      "2024-02-25 21:58:34.448862 ... noticias encontradas: 53\n",
      "2024-02-25 21:58:34.454604 ... noticias novas: 53\n",
      "2024-02-25 22:00:42.367696 ... textos buscados: 53\n",
      "2024-02-25 22:00:47.118566 ... novas noticias após filtros: 53\n",
      "\n",
      "Captura concluída\n"
     ]
    }
   ],
   "source": [
    "df_atual = pd.read_excel(base_noticias)\n",
    "df_empresas = pd.read_excel(lista_empresas)\n",
    "\n",
    "hash_noticias_ja_existentes = list(df_atual['hash'])\n",
    "\n",
    "# processa empresas\n",
    "for i, row in df_empresas[df_empresas['buscar'].str.upper() == 'SIM'].iterrows():\n",
    "    \n",
    "    empresa = row['empresa']\n",
    "    \n",
    "    aplicar_filtros= ( (not pd.isnull(row['aplicar_filtros_relevancia'])) and   row['aplicar_filtros_relevancia'].upper() == 'SIM')\n",
    "    inclui_apenas_nome_empresa = ( (not pd.isnull(row['tipo_busca_exaustiva'])) and    row['tipo_busca_exaustiva'].upper() == 'SIM')\n",
    "    \n",
    "    atualiza = (  (not pd.isnull(row['tipo_busca_atualizacao'])) and    row['tipo_busca_atualizacao'].upper() == 'SIM' and (not pd.isnull(row['ultima_data_publicacao'])))\n",
    "    ultima_data = row['ultima_data_publicacao'].date()\n",
    "    \n",
    "    print('\\n' +  str(dt.datetime.now())  + ' Buscando empresa ' + empresa)\n",
    "    dfEmpresasListadas = df_empresas\n",
    "    \n",
    "    df1 = busca_noticias_google_esg(empresa, atualiza=atualiza, ultima_data = ultima_data, inclui_apenas_nome_empresa = inclui_apenas_nome_empresa)\n",
    "    \n",
    "    if len(df1) >0:\n",
    "        # ajustes\n",
    "        df1['data_publicacao'] = df1['data_publicacao'].dt.date\n",
    "        df1 = ajusta_nomes_empresas_dataframe(df1)\n",
    "\n",
    "        # cria hash\n",
    "        df1['hash'] = df1.apply(lambda row: criar_hash_noticia(texto=row['titulo'], empresa=row['empresa'], titulo=row['titulo'], data=row['data_publicacao']), axis=1)\n",
    "        print(str(dt.datetime.now())  + ' ... noticias encontradas: ' + str(len(df1)))\n",
    "    \n",
    "    if len(df1) >0:\n",
    "        df1 = df1[~df1['hash'].isin(hash_noticias_ja_existentes)]\n",
    "        print(str(dt.datetime.now())  + ' ... noticias novas: ' + str(len(df1)))\n",
    "    \n",
    "    if len(df1) > 0:\n",
    "        df2 = recupera_noticias_completas(df1)\n",
    "        #print(df1)\n",
    "        print(str(dt.datetime.now())  + ' ... textos buscados: ' + str(len(df2)))\n",
    "            \n",
    "        if aplicar_filtros:\n",
    "            df3 = filtra_noticias_nao_relacionadas(df2, empresa)\n",
    "            if len(df3) >0:\n",
    "                df4 = filtra_citacao_relevante(df3, empresa, dfEmpresasListadas )\n",
    "                print(str(dt.datetime.now())  + ' ... novas noticias após filtro citaçoes: ' + str(len(df4)))\n",
    "                \n",
    "        else:\n",
    "            df4 = df2\n",
    "        \n",
    "        if len(df4) >0:\n",
    "            df5 = classifica_textos_coletados(df4)\n",
    "            df6 = df5\n",
    "            print(str(dt.datetime.now())  + ' ... novas noticias após filtros: ' + str(len(df6)))\n",
    "            df_atual = pd.read_excel(base_noticias)\n",
    "            df = pd.concat([df_atual,df6]).drop_duplicates(ignore_index=True)\n",
    "            df = df.drop_duplicates(['titulo','empresa','fonte', 'texto_completo'], keep='first')\n",
    "            df.to_excel(base_noticias, index=False)\n",
    "\n",
    "\n",
    "# complementando os dados\n",
    "# atualiza nome, cnpj, setor\n",
    "df_noticias = pd.read_excel(base_noticias)\n",
    "df_empresas = pd.read_excel(lista_empresas)\n",
    "df_noticias['Nome'] = df_noticias['empresa'].map(df_empresas.set_index('empresa')['Nome'])\n",
    "df_noticias['CNPJ'] = df_noticias['empresa'].map(df_empresas.set_index('empresa')['CNPJ'])\n",
    "df_noticias['Razão social'] = df_noticias['empresa'].map(df_empresas.set_index('empresa')['RAZÃO SOCIAL'])\n",
    "df_noticias['Código'] = df_noticias['empresa'].map(df_empresas.set_index('empresa')['Código'])\n",
    "df_noticias['Setor'] = df_noticias['empresa'].map(df_empresas.set_index('empresa')['SETOR'])\n",
    "df_noticias.to_excel(base_noticias, index=False)\n",
    "\n",
    "# atualiza ultima data de captura\n",
    "df_empresas['ultima_data_publicacao'] = df_empresas['empresa'].map(df_noticias[['empresa', 'data_publicacao']].groupby('empresa').max()['data_publicacao'] )\n",
    "df_empresas.to_excel(lista_empresas, index=False)\n",
    "\n",
    "print('\\nCaptura concluída')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1853bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806d850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d57b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c7599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1102e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
